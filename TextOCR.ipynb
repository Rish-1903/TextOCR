{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dfc13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of classes: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh-dang/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rishabh-dang/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/rishabh-dang/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████████████████████████████████| 44.7M/44.7M [00:01<00:00, 26.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [1/30], Loss: 1.7913488745689392\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [2/30], Loss: 1.7662411332130432\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [3/30], Loss: 1.7557042241096497\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [4/30], Loss: 1.7500055432319641\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [5/30], Loss: 1.7454574704170227\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [6/30], Loss: 1.7228506803512573\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [7/30], Loss: 1.6698459386825562\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [8/30], Loss: 1.6321247816085815\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [9/30], Loss: 1.60234934091568\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [10/30], Loss: 1.4729746580123901\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [11/30], Loss: 1.4194506406784058\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [12/30], Loss: 1.2651159167289734\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [13/30], Loss: 1.2365649938583374\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [14/30], Loss: 1.1039124727249146\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [15/30], Loss: 1.2115484178066254\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [16/30], Loss: 0.88385409116745\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [17/30], Loss: 0.7480770349502563\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [18/30], Loss: 0.8083329200744629\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [19/30], Loss: 0.5730436146259308\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [20/30], Loss: 0.489558681845665\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [21/30], Loss: 0.30117644369602203\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [22/30], Loss: 0.25183071196079254\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [23/30], Loss: 0.22766296565532684\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [24/30], Loss: 0.1447785198688507\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [25/30], Loss: 0.6385622024536133\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [26/30], Loss: 1.1312372162938118\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [27/30], Loss: 0.1342233344912529\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [28/30], Loss: 0.48524969816207886\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [29/30], Loss: 0.39847979694604874\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Epoch [30/30], Loss: 0.1211526058614254\n",
      "Shape after feature extractor: torch.Size([32, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 32, 256])\n",
      "Shape after feature extractor: torch.Size([9, 256, 4, 16])\n",
      "Shape before RNN: torch.Size([64, 9, 256])\n",
      "Accuracy: 97.5609756097561%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pdf2image import convert_from_path\n",
    "from torchvision import transforms\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert PDFs to images\n",
    "def pdfs_to_images(pdfs_folder, output_root):\n",
    "    if not os.path.exists(output_root):\n",
    "        os.makedirs(output_root)\n",
    "    \n",
    "    for pdf_name in os.listdir(pdfs_folder):\n",
    "        # Skip the specific PDF file\n",
    "        if pdf_name == \"PORCONES.228.35 – 1636.pdf\":\n",
    "            continue\n",
    "        \n",
    "        if pdf_name.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdfs_folder, pdf_name)\n",
    "            folder_name = pdf_name.replace(\".pdf\", \"\")\n",
    "            output_folder = os.path.join(output_root, folder_name)\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "            # Convert PDF to images\n",
    "            images = convert_from_path(pdf_path)\n",
    "            for i, image in enumerate(images):\n",
    "                image.save(f\"{output_folder}/page_{i+1}.jpg\", \"JPEG\")\n",
    "\n",
    "# Dataset class\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 256)),  # Increased resolution\n",
    "    transforms.Grayscale(),\n",
    "    transforms.RandomRotation(5),  # Small rotations\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Dataset class\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, image_root, label_encoder):\n",
    "        self.image_root = image_root\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        for folder_name in sorted(os.listdir(image_root)):  \n",
    "            folder_path = os.path.join(image_root, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for image_name in os.listdir(folder_path):\n",
    "                    if image_name.endswith(\".jpg\"):\n",
    "                        self.image_paths.append(os.path.join(folder_path, image_name))\n",
    "                        self.labels.append(folder_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)  # Convert to RGB for ResNet\n",
    "        image = transforms.ToPILImage()(image)\n",
    "        image = transform(image)\n",
    "        \n",
    "        # Encode label\n",
    "        label_encoded = self.label_encoder.transform([label])[0]\n",
    "        \n",
    "        return image, torch.tensor(label_encoded, dtype=torch.long)\n",
    "\n",
    "# Improved CRNN model with ResNet\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-3])  # Remove last few layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=256, hidden_size=512, num_layers=3, bidirectional=True)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)  # Feature extraction\n",
    "        print(f\"Shape after feature extractor: {x.shape}\")  # Debugging\n",
    "    \n",
    "        x = x.permute(0, 2, 3, 1)  # Change shape to (batch, height, width, channels)\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[-1])  # Flatten spatial dimensions (batch, seq_len, features)\n",
    "        x = x.permute(1, 0, 2)  # Convert to (seq_len, batch, feature_dim) for RNN\n",
    "    \n",
    "        print(f\"Shape before RNN: {x.shape}\")  # Debugging\n",
    "    \n",
    "        x, _ = self.rnn(x)  # Pass through RNN\n",
    "        x = self.fc(x.mean(dim=0))  # Fully connected layer (batch, num_classes)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs=30):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "# Convert PDFs to images\n",
    "pdfs_to_images(\"Pdfs\", \"images\")\n",
    "\n",
    "# Collect labels and encode\n",
    "all_labels = []\n",
    "for folder_name in os.listdir(\"images\"):\n",
    "    folder_path = os.path.join(\"images\", folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        all_labels.append(folder_name)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = OCRDataset(\"images\", label_encoder)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = CRNN(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1addd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
